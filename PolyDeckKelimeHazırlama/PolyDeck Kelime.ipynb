{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faf78dc-e65b-42fe-9802-eb724472bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#web kazıma ile cümle, kelime ve ses dosyalarının çıkartılması ve kaydedilmesi.\n",
    "path_en=\"html_kodları.txt\"\n",
    "output_path_en = \"output_en.json\"\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(path_en, \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "for line in content.split('\\n'):\n",
    "    if line.startswith('#') or not line.strip():\n",
    "        continue\n",
    "\n",
    "    fields = line.split('\\t')\n",
    "    if len(fields) < 6:\n",
    "        continue\n",
    "\n",
    "    word = fields[2]\n",
    "\n",
    "    html_content = fields[5].replace('\"\"', '\"')\n",
    "\n",
    "    sound_url = ''\n",
    "    if len(fields) > 6:\n",
    "        sound_match = re.search(r'https://.*?\\.mp3', fields[6])\n",
    "        if sound_match:\n",
    "            sound_url = sound_match.group(0)\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    examples = []\n",
    "    for li in soup.find_all('li', class_='content-example'):\n",
    "        example = li.text\n",
    "        example = re.sub(r'\\{\\{c1::(.*?)\\}\\}', r'\\1', example)\n",
    "        examples.append(example.strip())\n",
    "\n",
    "    data.append({\n",
    "        \"word\": word,\n",
    "        \"sound_url\": sound_url,\n",
    "        \"examples\": examples\n",
    "    })\n",
    "\n",
    "with open(output_path_en, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Tüm veriler '{output_path}' dosyasına başarıyla kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04ce76-7a70-4333-967e-9c0f3477ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sadece ingilizce kelime ve seviyelerin çıkartılması.\n",
    "import pandas as pd\n",
    "df=pd.read_excel(\"The Oxford 5000 - excel.xlsx\")\n",
    "import json\n",
    "\n",
    "level_list=[]\n",
    "en_list=[]\n",
    "for i in range(len(df)):\n",
    "    level_list.append(df[\"A1 \"][i])\n",
    "    en_list.append(df[\"a \"][i])\n",
    "    \n",
    "data=[{\"Level\":level,\"word\":en} for level,en in zip(level_list,en_list)]\n",
    "\n",
    "output_file = \"en_level.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "print(f\"Veriler '{output_file}' dosyasına başarıyla kaydedildi.\")\n",
    "\n",
    "with open(output_file, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def temizle_bosluklar(obj):\n",
    "    if isinstance(obj, str):\n",
    "        return obj.strip()\n",
    "    elif isinstance(obj, list):\n",
    "        return [temizle_bosluklar(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key.strip(): temizle_bosluklar(value) for key, value in obj.items()}\n",
    "    return obj\n",
    "\n",
    "temizlenmis_data = temizle_bosluklar(data)\n",
    "\n",
    "with open('en_level_son.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(temizlenmis_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fb08b-8778-4d81-8970-d3196d3a542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cümlelerin ve kelimelerin ayrı bulunduğu dosyalardaki kelimeler eşleştirildi ve ayrı bir dosyada kelime, seviye, cümle verileri kaydedildi.\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"en_level_son.json\", 'r', encoding='utf-8') as file: \n",
    "    en_level = json.load(file)\n",
    "with open(\"output_en.json\", 'r', encoding='utf-8') as file: \n",
    "    en_sentence = json.load(file)\n",
    "\n",
    "yeni_word = []\n",
    "yeni_sentence = []\n",
    "yeni_level = []\n",
    "buldu = 0\n",
    "\n",
    "for i in range(len(en_level)):\n",
    "    for k in range(len(en_sentence)):\n",
    "        if en_level[i][\"word\"] == en_sentence[k][\"word\"]:\n",
    "            buldu += 1\n",
    "            yeni_word.append(en_sentence[k][\"word\"])\n",
    "            yeni_level.append(en_level[i][\"Level\"])_l\n",
    "            if \"examples\" in en_sentence[k] and en_sentence[k][\"examples\"]:\n",
    "                yeni_sentence.append(en_sentence[k][\"examples\"][0])\n",
    "            else:\n",
    "                yeni_sentence.append(\"\")\n",
    "\n",
    "print(f\"Eşleşen kelime sayısı: {buldu}\")\n",
    "\n",
    "data = [{\"word\": word, \"sentence\": sentence, \"level\": level} \n",
    "        for word, sentence, level in zip(yeni_word, yeni_sentence, yeni_level)]\n",
    "\n",
    "unique_data = list({item['word']: item for item in data}.values())\n",
    "\n",
    "unique_data = sorted(unique_data, key=lambda x: x['word'])\n",
    "\n",
    "output_file = \"en_sentence_level.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(unique_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Benzersiz ve alfabetik sıralanmış veriler '{output_file}' dosyasına başarıyla kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b81279-1a22-4b19-9242-68be32771fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#çeviri işlemleri.\n",
    "import json\n",
    "import os\n",
    "with open(\"en_sentence_level.json\", 'r', encoding='utf-8') as file:\n",
    "    eng = json.load(file)\n",
    "\n",
    "languages = {\n",
    "    \"tr\": \"tr_sentence_level.json\",\n",
    "    \"de\": \"de_sentence_level.json\",\n",
    "    \"fr\": \"fr_sentence_level.json\",\n",
    "    \"es\": \"esp_sentence_level.json\",\n",
    "    \"it\": \"it_sentence_level.json\",\n",
    "    \"pt\": \"pr_sentence_level.json\"\n",
    "}\n",
    "\n",
    "for lang, output_file in languages.items():\n",
    "    translated_words = []\n",
    "    translated_sentences = []\n",
    "    translated_levels = []\n",
    "\n",
    "    for en in eng:\n",
    "        translated_word = GoogleTranslator(source='en', target=lang).translate(en[\"word\"])\n",
    "        translated_sentence = GoogleTranslator(source='en', target=lang).translate(en[\"sentence\"])\n",
    " \n",
    "        translated_words.append(translated_word)\n",
    "        translated_sentences.append(translated_sentence)\n",
    "        translated_levels.append(en[\"level\"])\n",
    "\n",
    "    data_translated = [\n",
    "        {\"word\": word, \"sentence\": sentence, \"level\": level}\n",
    "        for word, sentence, level in zip(translated_words, translated_sentences, translated_levels)\n",
    "    ]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data_translated, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Veriler '{output_file}' dosyasına başarıyla kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb293ee-a6ae-4c2e-af57-3acc9e86f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kelime sayısı kontrolü\n",
    "import os\n",
    "import json\n",
    "\n",
    "folder_path = \"C:/Users/onur_/Desktop/anki\"\n",
    "\n",
    "language_counts = {}\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        language = file_name.split(\"_\")[0]\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "            if language not in language_counts:\n",
    "                language_counts[language] = 0\n",
    "            language_counts[language] += len(data)\n",
    "\n",
    "print(\"Her dildeki toplam veri sayıları:\")\n",
    "for language, count in language_counts.items():\n",
    "    print(f\"  {language}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c549a4-e4cf-4677-9705-8d8e078f9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#klasördeki json dosyalarını seviyelerine göre sıralama ve id ekleme\n",
    "import os\n",
    "import json\n",
    "\n",
    "json_folder_path = \"C:/Users/onur_/Desktop/anki\"\n",
    "\n",
    "for file_name in os.listdir(json_folder_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_folder_path, file_name)\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        data = sorted(data, key=lambda x: x[\"level\"])\n",
    "        \n",
    "        for idx, item in enumerate(data):\n",
    "            item[\"id\"] = idx+1\n",
    "        \n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tüm dosyalar sıralandı ve id değerleri güncellendi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ed47b-2f61-4e23-83df-32f8837054ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kelimeleri indeksleme\n",
    "import os\n",
    "import json\n",
    "\n",
    "json_folder_path = \"C:/Users/onur_/Desktop/anki\"\n",
    "\n",
    "for file_name in os.listdir(json_folder_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_folder_path, file_name)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        for idx, item in enumerate(data):\n",
    "            item[\"id\"] = idx + 1\n",
    "        \n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tüm dosyalara index eklendi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b817a66-c08e-4505-a6e5-42443f15a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kelimeleri küçük harfelere dönüştürme ve cümlelerin ilk harflerini büyük yapma.\n",
    "import os\n",
    "import json\n",
    "\n",
    "json_folder_path = \"C:/Users/onur_/Desktop/anki\"\n",
    "\n",
    "for file_name in os.listdir(json_folder_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        file_path = os.path.join(json_folder_path, file_name)\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        for item in data:\n",
    "            item[\"word\"] = item[\"word\"].lower()\n",
    "            \n",
    "            if item.get(\"sentence\"):\n",
    "                item[\"sentence\"] = item[\"sentence\"].capitalize()\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Tüm dosyalar güncellendi.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
